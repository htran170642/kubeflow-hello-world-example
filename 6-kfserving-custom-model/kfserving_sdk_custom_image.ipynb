{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample for KFServing SDK with a custom image\n",
    "\n",
    "This is a sample for KFServing SDK using a custom image.\n",
    "\n",
    "The notebook shows how to use KFServing SDK to create, get and delete InferenceService with a custom image.\n",
    "\n",
    "### Setup\n",
    "- Your `~/.kube/config` should point to a cluster with KFServing installed.\n",
    "- Your cluster's Istio Ingress gateway must be network accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the docker image we will be using.\n",
    "\n",
    "The goal of custom image support is to allow users to bring their own wrapped model inside a container and serve it with KFServing. Please note that you will need to ensure that your container is also running a web server e.g. Flask to expose your model endpoints. This example extends kfserving.KFModel which uses the tornado web server.\n",
    "\n",
    "\n",
    "To build and push with Docker Hub set the `DOCKER_HUB_USERNAME` variable below with your Docker Hub username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Github: https://github.com/kserve/kserve/tree/v0.6.0/docs/samples/v1alpha2/custom/kfserving-custom-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to be your dockerhub username\n",
    "# It will be used when building your image and when creating the InferenceService for your image\n",
    "DOCKER_HUB_USERNAME = \"your_docker_username\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$DOCKER_HUB_USERNAME\"\n",
    "docker build -t $1/kfserving-custom-model ./model-server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$DOCKER_HUB_USERNAME\"\n",
    "docker push $1/kfserving-custom-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFServing Client SDK\n",
    "\n",
    "We will use the [KFServing client SDK](https://github.com/kubeflow/kfserving/blob/master/python/kfserving/README.md#kfserving-client) to create the InferenceService and deploy our custom image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client\n",
    "from kubernetes.client import V1Container\n",
    "\n",
    "from kfserving import KFServingClient\n",
    "from kfserving import constants\n",
    "from kfserving import utils\n",
    "from kfserving import V1alpha2EndpointSpec\n",
    "from kfserving import V1alpha2PredictorSpec\n",
    "from kfserving import V1alpha2InferenceServiceSpec\n",
    "from kfserving import V1alpha2InferenceService\n",
    "from kfserving import V1alpha2CustomSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = utils.get_default_target_namespace()\n",
    "print(namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define InferenceService\n",
    "\n",
    "Firstly define default endpoint spec, and then define the inferenceservice using the endpoint spec.\n",
    "\n",
    "To use a custom image we need to use V1alphaCustomSpec which takes a [V1Container](https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V1Container.md)\n",
    " from the kuberenetes library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_version = constants.KFSERVING_GROUP + '/' + constants.KFSERVING_VERSION\n",
    "\n",
    "default_endpoint_spec = V1alpha2EndpointSpec(\n",
    "                          predictor=V1alpha2PredictorSpec(\n",
    "                              custom=V1alpha2CustomSpec(\n",
    "                                  container=V1Container(\n",
    "                                      name=\"kfserving-custom-model\",\n",
    "                                      image=f\"{DOCKER_HUB_USERNAME}/kfserving-custom-model\"))))\n",
    "\n",
    "isvc = V1alpha2InferenceService(api_version=api_version,\n",
    "                          kind=constants.KFSERVING_KIND,\n",
    "                          metadata=client.V1ObjectMeta(\n",
    "                              name='kfserving-custom-model', namespace=namespace),\n",
    "                          spec=V1alpha2InferenceServiceSpec(default=default_endpoint_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the InferenceService\n",
    "\n",
    "Call KFServingClient to create InferenceService."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KFServing = KFServingClient()\n",
    "KFServing.create(isvc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the InferenceService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KFServing.get('kfserving-custom-model', namespace=namespace, watch=True, timeout_seconds=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"kfserving-custom-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --out CLUSTER_IP\n",
    "INGRESS_GATEWAY=\"istio-ingressgateway\"\n",
    "echo \"$(kubectl -n istio-system get service $INGRESS_GATEWAY -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL_NAME\" --out SERVICE_HOSTNAME\n",
    "echo \"$(kubectl get inferenceservice $1 -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 192.168.49.2:31225...\n",
      "* TCP_NODELAY set\n",
      "* Connected to 192.168.49.2 (192.168.49.2) port 31225 (#0)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "* Mark bundle as not supporting multiuse\n",
      "\n",
      "* We are completely uploaded and fine\n",
      "* Mark bundle as not supporting multiuse\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "* Connection #0 to host 192.168.49.2 left intact\n",
      "{\"predictions\": {\"Labrador retriever\": 0.41585126519203186, \"golden retriever\": 0.16591677069664001, \"Saluki, gazelle hound\": 0.1628689467906952, \"whippet\": 0.028539132326841354, \"Ibizan hound, Ibizan Podenco\": 0.02392476797103882}}"
     ]
    }
   ],
   "source": [
    "!curl -v http://192.168.49.2:31225/v1/models/kfserving-custom-model:predict -d @./input.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"predictions\": {\"Labrador retriever\": 0.41585126519203186, \"golden retriever\": 0.16591677069664001, \"Saluki, gazelle hound\": 0.1628689467906952, \"whippet\": 0.028539132326841354, \"Ibizan hound, Ibizan Podenco\": 0.02392476797103882}}'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "with open('input.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    url =\"http://192.168.49.2:31225/v1/models/kfserving-custom-model:predict\"\n",
    "    result = requests.post(url, data=json.dumps(data))\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:         kfserving-custom-model\n",
      "Namespace:    default\n",
      "Labels:       controller-tools.k8s.io=1.0\n",
      "Annotations:  <none>\n",
      "API Version:  serving.kubeflow.org/v1beta1\n",
      "Kind:         InferenceService\n",
      "Metadata:\n",
      "  Creation Timestamp:  2022-03-12T08:13:29Z\n",
      "  Finalizers:\n",
      "    inferenceservice.finalizers\n",
      "  Generation:  2\n",
      "  Managed Fields:\n",
      "    API Version:  serving.kubeflow.org/v1alpha2\n",
      "    Fields Type:  FieldsV1\n",
      "    fieldsV1:\n",
      "      f:metadata:\n",
      "        f:annotations:\n",
      "          .:\n",
      "          f:kubectl.kubernetes.io/last-applied-configuration:\n",
      "        f:labels:\n",
      "          .:\n",
      "          f:controller-tools.k8s.io:\n",
      "      f:spec:\n",
      "        .:\n",
      "        f:default:\n",
      "          .:\n",
      "          f:predictor:\n",
      "            .:\n",
      "            f:custom:\n",
      "              .:\n",
      "              f:container:\n",
      "                .:\n",
      "                f:image:\n",
      "                f:name:\n",
      "    Manager:      kubectl-client-side-apply\n",
      "    Operation:    Update\n",
      "    Time:         2022-03-12T08:13:27Z\n",
      "    API Version:  serving.kubeflow.org/v1beta1\n",
      "    Fields Type:  FieldsV1\n",
      "    fieldsV1:\n",
      "      f:metadata:\n",
      "        f:finalizers:\n",
      "      f:spec:\n",
      "        f:predictor:\n",
      "          f:containers:\n",
      "      f:status:\n",
      "        f:address:\n",
      "          .:\n",
      "          f:url:\n",
      "        f:components:\n",
      "          .:\n",
      "          f:predictor:\n",
      "            .:\n",
      "            f:address:\n",
      "              .:\n",
      "              f:url:\n",
      "            f:latestCreatedRevision:\n",
      "            f:latestReadyRevision:\n",
      "            f:latestRolledoutRevision:\n",
      "            f:traffic:\n",
      "            f:url:\n",
      "        f:conditions:\n",
      "        f:url:\n",
      "    Manager:         manager\n",
      "    Operation:       Update\n",
      "    Time:            2022-03-12T08:15:40Z\n",
      "  Resource Version:  1277051\n",
      "  Self Link:         /apis/serving.kubeflow.org/v1beta1/namespaces/default/inferenceservices/kfserving-custom-model\n",
      "  UID:               f97a8461-769a-4811-9346-a1a5cecd97e2\n",
      "Spec:\n",
      "  Predictor:\n",
      "    Containers:\n",
      "      Image:  170642/kfserving-custom-model\n",
      "      Name:   kfserving-container\n",
      "      Resources:\n",
      "        Limits:\n",
      "          Cpu:     1\n",
      "          Memory:  2Gi\n",
      "        Requests:\n",
      "          Cpu:     1\n",
      "          Memory:  2Gi\n",
      "Status:\n",
      "  Address:\n",
      "    URL:  http://kfserving-custom-model.default.svc.cluster.local/v1/models/kfserving-custom-model:predict\n",
      "  Components:\n",
      "    Predictor:\n",
      "      Address:\n",
      "        URL:                      http://kfserving-custom-model-predictor-default.default.svc.cluster.local\n",
      "      Latest Created Revision:    kfserving-custom-model-predictor-default-00001\n",
      "      Latest Ready Revision:      kfserving-custom-model-predictor-default-00001\n",
      "      Latest Rolledout Revision:  kfserving-custom-model-predictor-default-00001\n",
      "      Traffic:\n",
      "        Latest Revision:  true\n",
      "        Percent:          100\n",
      "        Revision Name:    kfserving-custom-model-predictor-default-00001\n",
      "      URL:                http://kfserving-custom-model-predictor-default.default.example.com\n",
      "  Conditions:\n",
      "    Last Transition Time:  2022-03-12T08:15:40Z\n",
      "    Status:                True\n",
      "    Type:                  IngressReady\n",
      "    Last Transition Time:  2022-03-12T08:15:40Z\n",
      "    Severity:              Info\n",
      "    Status:                True\n",
      "    Type:                  PredictorConfigurationReady\n",
      "    Last Transition Time:  2022-03-12T08:15:40Z\n",
      "    Status:                True\n",
      "    Type:                  PredictorReady\n",
      "    Last Transition Time:  2022-03-12T08:15:39Z\n",
      "    Severity:              Info\n",
      "    Status:                True\n",
      "    Type:                  PredictorRouteReady\n",
      "    Last Transition Time:  2022-03-12T08:15:40Z\n",
      "    Status:                True\n",
      "    Type:                  Ready\n",
      "  URL:                     http://kfserving-custom-model.default.example.com\n",
      "Events:\n",
      "  Type     Reason                 Age    From                Message\n",
      "  ----     ------                 ----   ----                -------\n",
      "  Warning  InternalError          4m25s  v1beta1Controllers  fails to reconcile predictor: fails to update knative service: Operation cannot be fulfilled on services.serving.knative.dev \"kfserving-custom-model-predictor-default\": the object has been modified; please apply your changes to the latest version and try again\n",
      "  Warning  UpdateFailed           2m20s  v1beta1Controllers  Failed to update status for InferenceService \"kfserving-custom-model\": Operation cannot be fulfilled on inferenceservices.serving.kubeflow.org \"kfserving-custom-model\": the object has been modified; please apply your changes to the latest version and try again\n",
      "  Warning  InternalError          2m20s  v1beta1Controllers  fails to update InferenceService status: Operation cannot be fulfilled on inferenceservices.serving.kubeflow.org \"kfserving-custom-model\": the object has been modified; please apply your changes to the latest version and try again\n",
      "  Normal   InferenceServiceReady  2m19s  v1beta1Controllers  InferenceService [kfserving-custom-model] is Ready\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe inferenceservice kfserving-custom-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Could not resolve host: kfserving-custom-model.default.svc.cluster.local\n",
      "* Closing connection 0\n",
      "curl: (6) Could not resolve host: kfserving-custom-model.default.svc.cluster.local\n"
     ]
    }
   ],
   "source": [
    "!curl -v http://kfserving-custom-model.default.svc.cluster.local/v1/models/kfserving-custom-model:predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the InferenceService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KFServing.delete(MODEL_NAME, namespace=namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
